# -*- coding: utf-8 -*-
"""Roberta_wTeamSentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xnW4Oe6BB8HyNd9KQTEgO4MAMrcJMfy-
"""

# Code inspired by https://towardsdatascience.com/multi-class-classification-with-transformers-6cf7b59a033a and https://github.com/jamescalam/transformers/blob/main/course/project_build_tf_sentiment_model/01_input_pipeline.ipynb

#!pip install transformers

#IMPORT LIBS
import tensorflow as tf
import numpy as np
from transformers import TFAutoModel,RobertaTokenizer, TFRobertaModel, RobertaModel
from tensorflow.keras.callbacks import EarlyStopping
from tqdm import tqdm
from sklearn.metrics import f1_score,confusion_matrix,accuracy_score,recall_score,precision_score,classification_report
from keras.models import load_model
import re
import pandas as pd


#GET RAW DATA - Called data.csv when uploaded to GoogleCollab after running C:\Users\ayman\Documents\GitHub\GT\DVA\Team_094\SentimentModelScripts\Sample\2 - data_rejoinier.py
df = pd.read_csv('data.csv') # pd.read_csv('Data/WSB_Comments_Clean_wClusters_Reduced.csv')
df = df.fillna('')
print(df.head())

#PARTITION AND SPLIT DATA
df2 = df[['ProcessedComments', 'HumanSentiment']]

test = df['HumanSentiment'].value_counts()

df2['sentiment_numeric']  = pd.factorize(df2["HumanSentiment"])[0]

dictionary = pd.Series(df2["HumanSentiment"].values,index=df2['sentiment_numeric']).to_dict()
print(dictionary)
np.save('sentiments.npy', dictionary)

df2 = df2[['ProcessedComments', 'sentiment_numeric']]
print(df2.head())
print(len(df2))

from sklearn.model_selection import train_test_split
df_subset_train_model, df_subset_val_model = train_test_split(df2, test_size=0.1)


df2 = df_subset_train_model[['ProcessedComments','sentiment_numeric']]
print(len(df2))

#%ROBERTA SETUP
seq_len = 512
num_samples = len(df2)

Xids = np.zeros((num_samples, 512))
Xmask = np.zeros((num_samples, 512))

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

#from transformers import AutoTokenizer, TFAutoModel
#tokenizer = AutoTokenizer.from_pretrained("kamalkraj/deberta-base")

for i, phrase in enumerate(df2['ProcessedComments']):
    tokens = tokenizer.encode_plus(phrase, max_length=seq_len, truncation=True,
                                   padding='max_length', add_special_tokens=True,
                                   return_tensors='tf')
    Xids[i, :] = tf.cast(tokens['input_ids'],tf.float64)
    Xmask[i, :] = tf.cast(tokens['attention_mask'],tf.float64)

arr = df2['sentiment_numeric'].values
print(arr)

labels = np.zeros((num_samples, arr.max()+1))
print(labels.shape)

labels[np.arange(num_samples), arr] = 1

#PREPARE DATA FOR TF MODEL
import tensorflow as tf

dataset = tf.data.Dataset.from_tensor_slices((Xids,Xmask,labels))

def map_func(input_ids, masks, labels):
    return {'input_ids': input_ids, 'attention_mask': masks}, labels

dataset = dataset.map(map_func)

batch_size = 13
dataset = dataset.shuffle(10000).batch(batch_size,drop_remainder=True)

split = 0.80
size = int((num_samples/batch_size) * split)

train_ds = dataset.take(size)
val_ds = dataset.skip(size)
print(train_ds)
print(val_ds)
del dataset

#%%RUN TF MODEL 100 EPOCHS

from transformers import TFAutoModel,RobertaTokenizer, TFRobertaModel
from tensorflow.keras.callbacks import EarlyStopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, min_delta = 0.1, verbose = 1)

bert = TFRobertaModel.from_pretrained("roberta-base")
print(bert.summary())

input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')
mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')

embeddings = bert.roberta(input_ids, attention_mask=mask)[1]

x = tf.keras.layers.Dense(1024,activation='relu')(embeddings)
y = tf.keras.layers.Dense(arr.max()+1,activation='softmax', name = 'outputs')(x)

model = tf.keras.Model(inputs=[input_ids,mask], outputs=y)
print(model.summary())

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-7,decay=1e-6)
loss = tf.keras.losses.CategoricalCrossentropy()
acc = tf.keras.metrics.CategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss = loss, metrics = [acc])

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs = 100#,
    #callbacks=[early_stopping]
)

#SAVE MODEL
model.save('sentiment_model_teamsentiment')

!zip -r /content/sentiment_model_teamsentiment.zip /content/sentiment_model_teamsentiment

from google.colab import files
files.download("/content/sentiment_model_teamsentiment.zip")

#%% TEST MODEL

from sklearn.metrics import classification_report

def prep_data(text):
    tokens = tokenizer.encode_plus(text, max_length=512, truncation=True,
                                   padding='max_length', add_special_tokens=True,#return_token_type_id=False,
                                   return_tensors='tf')
    return{
        'input_ids': tf.cast(tokens['input_ids'], tf.float64),
        'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)
        }

#test = prep_data('I hate all this')
#test = prep_data('GME to the Moon!')
#test = prep_data('I love this')
probs = model.predict(test)

np.argmax(probs[0])

# TEST MODEL ON VALIDATION DATA
df_subset_val_model = df_subset_val_model.reset_index(drop=True)
#del df_subset_val_model['index']
df_subset_val_model.head()

cor = df_subset_val_model['ProcessedComments'].apply(prep_data)


pred = [np.argmax((model.predict(cor[i]))[0]) for i in tqdm(range(len(cor)))]
pred2 = [model.predict(cor[i]) for i in tqdm(range(len(cor)))]
df_subset_val_model['predsentiment'] = pred
df_subset_val_model['predsentiment2'] = pred2

print(df_subset_val_model.head())

#%% CLASSIFICATION REPORT
print('\nclassification report:\n', classification_report(df_subset_val_model['sentiment_numeric'],df_subset_val_model['predsentiment']))

df_confusion = pd.crosstab(df_subset_val_model['sentiment_numeric'], df_subset_val_model['predsentiment'], rownames=['Actual'], colnames=['Predicted'], margins=True)
print(df_confusion)

df_subset_val_model.predsentiment2.apply(str).value_counts()

df_subset_train_model = df_subset_train_model.reset_index(drop=True)
#del df_subset_val_model['index']
df_subset_train_model.head()

cor = df_subset_train_model['ProcessedComments'].apply(prep_data)


pred = [np.argmax((model.predict(cor[i]))[0]) for i in tqdm(range(len(cor)))]
pred2 = [model.predict(cor[i]) for i in tqdm(range(len(cor)))]
df_subset_train_model['predsentiment'] = pred
df_subset_train_model['predsentiment2'] = pred2

print(df_subset_train_model.head())

#%%
print('\nclassification report:\n', classification_report(df_subset_train_model['sentiment_numeric'],df_subset_train_model['predsentiment']))

df_confusion = pd.crosstab(df_subset_train_model['sentiment_numeric'], df_subset_train_model['predsentiment'], rownames=['Actual'], colnames=['Predicted'], margins=True)
print(df_confusion)

print(df_subset_train_model.predsentiment2.apply(str).value_counts())
